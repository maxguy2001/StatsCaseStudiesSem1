---
title: "Dementia Risk Factors: EDA of easySHARE data"
author: "Katrina Sanders, Kim Johnston, Max Guy"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load data

Load the easySHARE data. For information on data and variables, see the [easySHARE data  guide](http://www.share-project.org/fileadmin/pdf_documentation/easySHARE_Release_8.0.0_ReleaseGuide.pdf).

```{r load_data}
load("../data/easySHARE_rel8_0_0.rda")
dat<-easySHARE_rel8_0_0
rm(easySHARE_rel8_0_0)
```

Load packages:
  
```{r load_libs, message = FALSE}
library(tidyverse)
library(gridExtra)
#library(visdat)
#library(StatCompLab)
```

## Pre-processing

## Cognitive score

EasySHARE does not record diagnosis of dementia (e.g. Alzheimer's disease) in all waves. Instead, we will create a composite cognitive score as a proxy for dementia severity, following [Crimmins et al. (2011)](https://pubmed.ncbi.nlm.nih.gov/21743047/). The cognitive function indices are described in pg. 23 of the easySHARE data guide.

```{r cogindices, message = FALSE, fig.height=3, fig.width=9}
#select relevant cognitive variables
cogvars <- c("recall_1", "recall_2", "orienti", "numeracy_1", "numeracy_2")

#select cognitive subset of dataframe
cog = dat[cogvars]

#make exploratory plots
p1 = ggplot() +
  geom_histogram(aes(x=cog$recall_1), color="darkblue", fill="lightblue") +
  labs(x ="Recall of words, first trial")
p2 = ggplot() +
  geom_histogram(aes(x=cog$recall_2), color="darkblue", fill="lightblue") +
  labs(x ="Recall of words, second trial")
p3 = ggplot() +
  geom_histogram(aes(x=cog$orienti), color="darkblue", fill="lightblue") +
  labs(x ="Orientation to date")
p4 = ggplot() +
  geom_histogram(aes(x=cog$numeracy_1), color="darkblue", fill="lightblue") +
  labs(x ="Numeracy Score 1 (percentage)")
p5 = ggplot() +
  geom_histogram(aes(x=cog$numeracy_2), color="darkblue", fill="lightblue") +
  labs(x ="Numeracy Score 2 (subtraction)")
grid.arrange(p1, p2, p3, p4, p5, nrow = 2)
rm(p1, p2, p3, p4, p5)
```

Notice the missing values (missing codes are explained on pg. 6 of missing codes of the easySHARE guide). In addition, for the numeracy measures, only one score is recorded in latter waves (pg. 23). Here we take a simple average if both measures are available. However, you may consider other approaches (please include an explanation/motivation). Notice that  `numeracy_1` ranges from 1 to 5, while `numeracy_2` ranges from 0 to 5.

```{r numeracy, message = FALSE, warning=FALSE, fig.height=2, fig.width=3}
#function to add numeracy column
add_numeracy <- function(cog){
  numeracy = rep(NA,dim(cog)[1])
  numeracy[cog$numeracy_1 >= 0 & cog$numeracy_2 >= 0] =      (cog$numeracy_1[cog$numeracy_1 >= 0 & cog$numeracy_2 >= 0] + cog$numeracy_2[cog$numeracy_1 >= 0 & cog$numeracy_2 >= 0] )/2
  numeracy[cog$numeracy_1 >= 0 & cog$numeracy_2 < 0] = cog$numeracy_1[cog$numeracy_1 >= 0 & cog$numeracy_2 < 0]
  numeracy[cog$numeracy_1 < 0 & cog$numeracy_2 >= 0] = cog$numeracy_2[cog$numeracy_1 < 0 & cog$numeracy_2 >= 0]
  cog$numeracy = numeracy
  return(cog)
}

#call function to add numeracy column
cog <- add_numeracy(cog)

#plot numeracy histogram
ggplot() +
  geom_histogram(aes(x=cog$numeracy), color="darkblue", fill="lightblue") +
  labs(x ="Numeracy Score")
```

We now create a composite cognitive score, ranging from 0 (bad) to 29 (good), that combines the recall scores, orientation, and numeracy measure. Note that orientation the description of the scale of orientation in the easySHARE guide appears incorrect, and I believe it should be 0 (bad) and 4 (good). 

```{r cogscore, message = FALSE, warning=FALSE, fig.height=2, fig.width=3}

#function to add cognitive score
add_cogscore <- function(cog){
  numeracy <- cog$numeracy
  cogscore = rep(NA,dim(cog)[1])
  ind <<- cog$recall_1 >= 0 & cog$recall_2 >= 0 & cog$orienti >= 0 & !is.na(numeracy)
  cogscore[ind] = cog$recall_1[ind] + cog$recall_2[ind] +  cog$orienti[ind] + cog$numeracy[ind]
  cog$cogscore = cogscore
return(cog)
}

#call function to add cognitive score
cog <- add_cogscore(cog)

#plot cognitive score histogram
ggplot() +
  geom_histogram(aes(x=cog$cogscore[ind]), color="darkblue", fill="lightblue", binwidth =2 ) +
  labs(x ="Composite cogntive score")

dat$cogscore<-cog$cogscore
```


## Dementia risk factors

Extract variables related to modifiable risk factors identified in literature: education, hearing loss, traumatic brain injury, hypertension, alcohol consumption, smoking, obesity, physical activity, depression, social isolation, diabetes, and air pollution [Livingston et al. (2020)](https://www.thelancet.com/article/S0140-6736(20)30367-6/fulltext). Refer to the easySHARE guide for a description of the variables availble. You may also consider other relevant risk factors, along with an explanation and motivation. Gender and age are also relevant variables to consider, e.g. to explore differences between men and women [Beam et al. (2018)](https://pubmed.ncbi.nlm.nih.gov/30010124/).


```{r filtering_df}
#filter subset of dataframe
df <- dat %>%
  filter(country == 35) %>%
  filter(wave == 4)

#set appropriate na values
df[df < 0 ] <- NA

df$everyday_tasks <- df$adla + df$iadlza

df$diabetes <- df$chronic_mod
df$hypertension <- df$chronic_mod

# adding hypertension and diabetes to the variables
df$diabetes[df$diabetes != 5] <- 0
df$diabetes[df$diabetes == 5] <- 1
df$hypertension[df$hypertension != 2] <- 0
df$hypertension[df$hypertension == 2] <- 1
df$ever_smoked[df$ever_smoked == 5] <- 0


# removing variables we don't want or have too many NAs

df_columns <- df[,(names(df) %in% c("female", "age", "sphus", "isced1997_r", "eduyears_mod", "hhsize", "euro5", "eurod", "hypertension", "br010_mod", "bmi", "ever_smoked", "br015_", "mobilityind", "diabetes", "everyday_tasks", "cogscore"))]

# now remove the ones that are part of cognitive score
df_columns <- df_columns[,!(names(df_columns) %in% c( "numeracy_1", "numeracy_2", "orienti", "recall_1", "recall_2"))]

colSums(is.na(df_columns))

# count number of usable rows
nrow(df_columns %>% drop_na())

#remove incomplete rows from dataframe
df_final <- na.omit(df_columns)

```

## Exploring data, stats:


When first looking at the data, we wanted to clearly be able to see if there was any problematic data we would have to deal with. This graph shows the amount of NAs and so from here we are able to look at the mostly NA columns closer to decide if they need to be kept in to further our investigation.
```{r}
#vis_dat(df)
```


By first instinct, mother_alive and father_alive we would think that these could confound with age. So from exploring the relationship,  this graph clearly shows that as the mean of the distribution of age is higher for when mother_alive is yes.

Confounding variables:

```{r mother_alive}
boxplot(df$age ~ df$mother_alive, 
        data = df, 
        main="Corelation between Age and Mother Alive", 
        xlab= "age", 
        ylab= "mother_alive")

```

This shows it for father_alive, by the proportion of if the observations age is greater than 60, then there is only a small proportion with their father still alive.

```{r proportion_father_alive}
plot(prop.table(xtabs(~ (round(age)>60) +father_alive, df),2), 
     main="Proportion of Age >60, of Father Alive", 
     xlab="age > 60", 
     col=c("forestgreen", "dodgerblue"))

```


Its clear that the age of chronic mod, is much higher and so we can conclude that chronic_mod confounds with age.

```{r age_chronic_mod}
boxplot(df$age ~ df$chronic_mod, 
        data = df, 
        main="Corelation between Age and Chronic Disease", 
        xlab= "age", 
        ylab= "chronic_mod")

```


By this simple box plot clear that the distribution of age is much higher. We can then say that cognitive score once calculated wont be affected by an individual being young as there is no age observation that will skew this. Hence also concluding that when looking at dementia it is clear that it is only affected in older ages.

```{r hist_of_ages}

barplot(table(round(df_final$age)), 
        main="Histogram of Ages", 
        col = "forestgreen")

```



This graph shows there appears to be a a pattern between those who have smoked and the age of participants,  particularly the older participants. This may suggest that smoking has some impact on life expectancy but would need further investigation.

```{r}

boxplot(round(df_final$age) ~ df_final$ever_smoked, 
        data = df_final, 
        main="Correlation between Age and Smoking", 
        xlab= "ever_smoked", 
        ylab= "age")


```



Clear trend that as age decreases the cogscore does too.
```{r cogscore_age}
ggplot(df_final, aes(x=age, y=cogscore)) +
  geom_point(alpha=0.2) + 
  geom_smooth(method="lm", se=FALSE, color = "red", formula=y~x)

```

```{r self_health}

boxplot(round(df_final$cogscore) ~ round(df_final$sphus) , 
        data = df_final, 
        main="Correlation between Cognitive score and self-percieved health", 
        xlab= "sphus", 
        ylab= "cogscore")
```

```{r education_trend}
boxplot(round(df_final$cogscore) ~ round(df_final$isced1997_r) , 
        data = df_final, 
        main="Correlation between Cognitive score and Level of education acheived", 
        xlab= "isced1997_r", 
        ylab= "cogscore")

```


## Creating a model


```{r model_seup}
set.seed(567)

train_index <- sample(1:nrow(df_final), size =200)

df_test <- df_final[train_index, ]
df_train <- df_final[-train_index,]

#check there is no overlap between data sets
dplyr::intersect(df_train, df_test) 
```


```{r construct_model}
lm <- lm(cogscore ~ ., data = df_train)
summary(lm)

lm1 <- lm(cogscore ~ age*(female + age + isced1997_r + eduyears_mod + hhsize + 
    sphus + euro5 + eurod + mobilityind + bmi + ever_smoked + br010_mod + br015_ + diabetes + 
    hypertension + everyday_tasks), data = df_train)  

summary(lm1)

```

```{r update_model_1}

model<-step(lm1, trace=F, k=log(nrow(df_train))) # BIC, Bayesian Information criterion. This penalises more for large sample size. Hence we end up with a smaller model.
summary(model)
formula(model)
plot(model)

vars <- c("cogscore", "female", "age", "isced1997_r", "eduyears_mod", "sphus", "euro5", "eurod", "br015_", "everyday_tasks" )
```

```{r bic_interactions}
model_interactions<-step(lm1, trace=F, k=log(nrow(df_train))) 
summary(model_interactions)
formula(model_interactions)
plot(model_interactions)
```



```{r upate_model_2}
new_model <- lm(cogscore ~ age*(female + age + isced1997_r + eduyears_mod + euro5 + 
    eurod + everyday_tasks ), data = df_train)

new_model<-step(new_model, trace=F, k=log(nrow(df_train))) # BIC, Bayesian Information criterion. This penalises more for large sample size. Hence we end up with a smaller model.
summary(new_model)
formula(new_model)
plot(new_model)
```

```{r baseline_model}

model_age <- lm(cogscore ~ age, data = df_train)

summary(model_age)
formula(model_age)
plot(model_age)
```

```{r getting_rmse}
# residual sum of squares 
RSS <- c(crossprod(model$residuals))
RSS_new <- c(crossprod(new_model$residuals))
RSS_interactions <- c(crossprod(model_interactions$residuals))
RSS_age <- c(crossprod(model_age$residuals))

# mean square error
MSE <- RSS/length(model$residuals)
MSE_new <- RSS_new/length(new_model$residuals)
MSE_interactions <- RSS_interactions/length(model_interactions$residuals)
MSE_age <- RSS_age/length(model_age$residuals)

# root mean square error
RMSE <- sqrt(MSE)
RMSE_new <- sqrt(MSE_new)
RMSE_interactions <- sqrt(MSE_interactions)
RMSE_age <- sqrt(MSE_age)

```


```{r producing_ci}
# to include in table
coefs<-model_interactions$coefficients 
CI<- confint(model_interactions) 
print(CI)
```



```{r plotting_model}
ggplot(df_train, aes(x = 0.9016* female - 0.1277* age + 0.6852*isced1997_r + 0.1493*eduyears_mod - 0.2282*sphus + 0.5693*euro5 - 0.2956*eurod - 0.1497*br015_  -0.4554*everyday_tasks, y = cogscore)) +
  geom_point() +
  stat_smooth()
```


# Validation
```{r sorting-dataframes}

#TODO(max): make this use less variables!
df_train_validation <- df_train[, names(df_train) %in% vars]
df_test_validation <- df_test[, names(df_test) %in% vars]

dat_country_28 <- dat[dat$country == 28,]
df_w4_c28 <- dat_country_28[dat_country_28$wave == 4,]

dat_country_35 <- dat[dat$country == 35,]
df_w6_c35 <- dat_country_35[dat_country_35$wave == 6,]

df_w6_c35[df_w6_c35 < 0 ] <- NA
df_w4_c28[df_w4_c28 < 0 ] <- NA

df_w6_c35 <- df_w6_c35[, names(df_w6_c35) %in% vars]
df_w4_c28 <- df_w4_c28[, names(df_w4_c28) %in% vars]

# count unusable rows
nrow(df_w6_c35 %>% drop_na())
nrow(df_w4_c28 %>% drop_na())

# drop unusable rows
df_w6_c35 <- na.omit(df_w6_c35)
df_w4_c28 <- na.omit(df_w4_c28)

```


```{r plotting_differences}
p_train <- data.frame(actual = df_train$cogscore, predicted = predict(model))
head(p_train)
plot(p_train$actual, p_train$predicted)
```


```{r validation_test}
p_test <- predict(model, df_test_validation)

print(head(p_test))

error_test <- p_test - df_test_validation[["cogscore"]]
RMSE_test <- sqrt(mean(error_test^2))

plot(error_test)


print(RMSE_test)

```



##Predictions - scoring
This is taken from Statistical Computing:
```{r scoring}
library(StatCompLab)

mean <-mean(df_final$cogscore)
sd <- sd(df_final$cogscore)
lwr <- mean - sd * qnorm(0.9)
upr <- mean - sd * qnorm(0.1)

#proper score is defined in statcomplab
score_A <- cbind(RMSE, df_train) %>%
  mutate(
    se = proper_score("se", cogscore, mean = mean(cogscore)),
    ds = proper_score("ds", cogscore, mean = mean(cogscore), sd = sd(cogscore)),
    interval = proper_score("interval", cogscore,
                            lwr = lwr, upr = upr, alpha = 0.1)
  )

score_A_table <-
  rbind(cbind(score_A, model = "A")) %>%
  group_by(model) %>%
          summarise(se = mean(se),
                    ds = mean(ds),
                    interval = mean(interval))
knitr::kable(score_A_table)
```


```{r scoring}
score_B <- cbind(RMSE_test, df_train) %>%
  mutate(
    se = proper_score("se", cogscore, mean = mean(cogscore)),
    ds = proper_score("ds", cogscore, mean = mean(cogscore), sd = sd(cogscore)),
    interval = proper_score("interval", cogscore,
                            lwr = lwr, upr = upr, alpha = 0.1)
  )

score_B_table <-
  rbind(cbind(score_B, model = "A")) %>%
  group_by(model) %>%
          summarise(se = mean(se),
                    ds = mean(ds),
                    interval = mean(interval))
knitr::kable(score_B_table)
```




