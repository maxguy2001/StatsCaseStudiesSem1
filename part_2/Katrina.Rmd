---
title: "Dementia Risk Factors: EDA of easySHARE data"
author: "Katrina Sanders"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load data

Load the easySHARE data. For information on data and variables, see the [easySHARE data  guide](http://www.share-project.org/fileadmin/pdf_documentation/easySHARE_Release_8.0.0_ReleaseGuide.pdf).

```{r easyshare}
load("../data/easySHARE_rel8_0_0.rda")
dat<-easySHARE_rel8_0_0
```

Load packages:
  
```{r packs, message = FALSE}
library(ggplot2)
library(tidyverse)
library(gridExtra)
```

## Pre-processing

## Cognitive score

EasySHARE does not record diagnosis of dementia (e.g. Alzheimer's disease) in all waves. Instead, we will create a composite cognitive score as a proxy for dementia severity, following [Crimmins et al. (2011)](https://pubmed.ncbi.nlm.nih.gov/21743047/). The cognitive function indices are described in pg. 23 of the easySHARE data guide.

```{r cogindices, message = FALSE, fig.height=3, fig.width=9}

cogvars <- c("recall_1", "recall_2", "orienti", "numeracy_1", "numeracy_2")
cog = dat[cogvars]
p1 = ggplot() +
  geom_histogram(aes(x=cog$recall_1), color="darkblue", fill="lightblue") +
  labs(x ="Recall of words, first trial")
p2 = ggplot() +
  geom_histogram(aes(x=cog$recall_2), color="darkblue", fill="lightblue") +
  labs(x ="Recall of words, second trial")
p3 = ggplot() +
  geom_histogram(aes(x=cog$orienti), color="darkblue", fill="lightblue") +
  labs(x ="Orientation to date")
p4 = ggplot() +
  geom_histogram(aes(x=cog$numeracy_1), color="darkblue", fill="lightblue") +
  labs(x ="Numeracy Score 1 (percentage)")
p5 = ggplot() +
  geom_histogram(aes(x=cog$numeracy_2), color="darkblue", fill="lightblue") +
  labs(x ="Numeracy Score 2 (subtraction)")
grid.arrange(p1, p2, p3, p4, p5, nrow = 2)
```

Notice the missing values (missing codes are explained on pg. 6 of missing codes of the easySHARE guide). In addition, for the numeracy measures, only one score is recorded in latter waves (pg. 23). Here we take a simple average if both measures are available. However, you may consider other approaches (please include an explanation/motivation). Notice that  `numeracy_1` ranges from 1 to 5, while `numeracy_2` ranges from 0 to 5.

```{r numeracy, message = FALSE, warning=FALSE, fig.height=2, fig.width=3}
numeracy = rep(NA,dim(cog)[1])
numeracy[cog$numeracy_1 >= 0 & cog$numeracy_2 >= 0] = (cog$numeracy_1[cog$numeracy_1 >= 0 & cog$numeracy_2 >= 0] + cog$numeracy_2[cog$numeracy_1 >= 0 & cog$numeracy_2 >= 0] )/2
numeracy[cog$numeracy_1 >= 0 & cog$numeracy_2 < 0] = cog$numeracy_1[cog$numeracy_1 >= 0 & cog$numeracy_2 < 0]
numeracy[cog$numeracy_1 < 0 & cog$numeracy_2 >= 0] = cog$numeracy_2[cog$numeracy_1 < 0 & cog$numeracy_2 >= 0]
cog$numeracy = numeracy
ggplot() +
  geom_histogram(aes(x=cog$numeracy), color="darkblue", fill="lightblue") +
  labs(x ="Numeracy Score")
```

We now create a composite cognitive score, ranging from 0 (bad) to 29 (good), that combines the recall scores, orientation, and numeracy measure. Note that orientation the description of the scale of orientation in the easySHARE guide appears incorrect, and I believe it should be 0 (bad) and 4 (good). 

```{r cogscore, message = FALSE, warning=FALSE, fig.height=2, fig.width=3}
cogscore = rep(NA,dim(cog)[1])
ind = cog$recall_1 >= 0 & cog$recall_2 >= 0 & cog$orienti >= 0 & !is.na(numeracy)
cogscore[ind] = cog$recall_1[ind] + cog$recall_2[ind] +  cog$orienti[ind] + cog$numeracy[ind]
cog$cogscore = cogscore
ggplot() +
  geom_histogram(aes(x=cog$cogscore[ind]), color="darkblue", fill="lightblue", binwidth =2 ) +
  labs(x ="Composite cogntive score")

dat$cogscore<-cog$cogscore
```


## Dementia risk factors

Extract variables related to modifiable risk factors identified in literature: education, hearing loss, traumatic brain injury, hypertension, alcohol consumption, smoking, obesity, physical activity, depression, social isolation, diabetes, and air pollution [Livingston et al. (2020)](https://www.thelancet.com/article/S0140-6736(20)30367-6/fulltext). Refer to the easySHARE guide for a description of the variables availble. You may also consider other relevant risk factors, along with an explanation and motivation. Gender and age are also relevant variables to consider, e.g. to explore differences between men and women [Beam et al. (2018)](https://pubmed.ncbi.nlm.nih.gov/30010124/).


```{r}
dat_country_35 <- dat[dat$country == 35,]

df <- dat_country_35[dat_country_35$wave == 4,]

df[df < 0 ] <- NA

df$everyday_tasks <- df$adla + df$iadlza

df$diabetes <- df$chronic_mod
df$hypertension <- df$chronic_mod

# adding hypertension and diabetes to the variables
df$diabetes[df$diabetes != 5] <- 0
df$diabetes[df$diabetes == 5] <- 1
df$hypertension[df$hypertension != 2] <- 0
df$hypertension[df$hypertension == 2] <- 1
df$ever_smoked[df$ever_smoked == 5] <- 0


# removing variables we don't want or have too many NAs

df_columns <- df[,(names(df) %in% c("female", "age", "sphus", "isced1997_r", "eduyears_mod", 
                                              "hhsize", "euro5", "eurod", "hypertension", "br010_mod", 
                                              "bmi", "ever_smoked", "br015_", "mobilityind", "diabetes",
                                              "everyday_tasks", "cogscore"))]


# now remove the ones that are part of cognitive score
df_columns <- df_columns[,!(names(df_columns) %in% c( "numeracy_1", "numeracy_2", "orienti", "recall_1", "recall_2"))]

colSums(is.na(df_columns))

# count number of usable rows
nrow(df_columns %>% drop_na())


```

female
age
isced1997_r
eduyears_mod
hhsize
sphus
diabetes
hypertension
euro5
eurod
mobilityind
bmi
ever_smoked
br010_mod
br015_
everyday_tasks
cogscore



## Creating a model

```{r}
df_final <- na.omit(df_columns)
```

```{r}
set.seed(567)

train_index <- sample(1:nrow(df_final), size =200)

df_test <- df_final[train_index, ]
df_train <- df_final[-train_index,]

#check there is no intercept
dplyr::intersect(df_train, df_test) 
```


```{r}
lm <- lm(cogscore ~ ., data = df_train)
formula(lm1)
# could paste this formula into lm below. Note gender * () fits main effects plus interaction for each combination

lm1 <- lm(cogscore ~ age*(female + age + isced1997_r + eduyears_mod + hhsize + 
    sphus + euro5 + eurod + mobilityind + bmi + ever_smoked + br010_mod + br015_ + diabetes + 
    hypertension + everyday_tasks), data = df_train)    
         
# alternative would be to select models separately for males and females. This might be the easier option. As besides interaction with gender there would also be other interactions. Which would get rather complicated in the above model. 

```

```{r }

model<-step(lm, trace=F, k=log(nrow(df_train))) # BIC, Bayesian Information criterion. This penalises more for large sample size. Hence we end up with a smaller model.
summary(model)
formula(model)
plot(model)

vars <- c("cogscore", "female", "age", "isced1997_r", "eduyears_mod", "sphus", "euro5", "eurod", "br015_", "everyday_tasks" )
```
```{r}
model_interactions<-step(lm1, trace=F, k=log(nrow(df_train))) 
summary(model_interactions)
formula(model_interactions)
plot(model_interactions)
```

```{r}
transform_model <- lm(cogscore ~ age*(female + age + isced1997_r + eduyears_mod + euro5 + 
    eurod + everyday_tasks ), data = df_train)

new_model<-step(transform_model, trace=F, k=log(nrow(df_train))) # BIC, Bayesian Information criterion. This penalises more for large sample size. Hence we end up with a smaller model.
summary(new_model)
formula(new_model)
plot(new_model)
```
```{r}
model_age <- lm(cogscore ~ age, data = df_train)

summary(model_age)
```

```{r}
# residual sum of squares 
RSS <- c(crossprod(model$residuals))
RSS_new <- c(crossprod(new_model$residuals))
RSS_interactions <- c(crossprod(model_interactions$residuals))
RSS_age <- c(crossprod(model_age$residuals))

# mean square error
MSE <- RSS/length(model$residuals)
MSE_new <- RSS_new/length(new_model$residuals)
MSE_interactions <- RSS_interactions/length(model_interactions$residuals)
MSE_age <- RSS_age/length(model_age$residuals)

# root mean square error
RMSE <- sqrt(MSE)
RMSE_new <- sqrt(MSE_new)
RMSE_interactions <- sqrt(MSE_interactions)
RMSE_age <- sqrt(MSE_age)
```

Low RMSE is good , higher on new model so doesn't predict as well. 

```{r }
# to include in table
coefs<-model_interactions$coefficients 
CI<- confint(model_interactions) 
print(CI)
```

```{r}
ggplot(df_train, aes(x = 0.9016* female - 0.1277* age + 0.6852*isced1997_r + 0.1493*eduyears_mod - 0.2282*sphus + 0.5693*euro5 - 0.2956*eurod - 0.1497*br015_  -0.4554*everyday_tasks, y = cogscore)) +
  geom_point() +
  stat_smooth()+
  xlab('predictive variables') +
  ggtitle('Plot of the linear model for predicting cogscore')
```




# Validation
```{r sorting-dataframes}

df_train_validation <- df_train[, names(df_train) %in% vars]
df_test_validation <- df_test[, names(df_test) %in% vars]

data_vars <- dat
data_vars$everyday_tasks <- data_vars$adla + data_vars$iadlza

data_35 <- data_vars[data_vars$country == 35,]
data_35_6 <- data_35[data_35$wave == 6,]

data_28 <- data_vars[data_vars$country == 28,]
data_28_4 <- data_28[data_28$wave == 4,]

data_35_6 <- data_35_6[, names(data_35_6) %in% vars]
data_35_6[data_35_6 < 0 ] <- NA

data_28_4 <- data_28_4[, names(data_28_4) %in% vars]
data_28_4[data_28_4 < 0 ] <- NA




# count unusable rows
nrow(data_35_6 %>% drop_na())
nrow(data_28_4 %>% drop_na())

# drop unusable rows
data_35_6 <- na.omit(data_35_6)
data_28_4 <- na.omit(data_28_4)

```


```{r}
p_train <- data.frame(actual = df_train$cogscore, predicted = predict(model))
head(p_train)
plot(p_train$actual, p_train$predicted)
```


```{r validation}
p_test <- predict(model, df_test_validation)
p_data_35_6 <- predict(model, data_35_6)
p_data_28_4 <- predict(model, data_28_4)

error_test <- p_test - df_test_validation[["cogscore"]]
RMSE_test <- sqrt(mean(error_test^2))

error_data_35_6 <- p_data_35_6 - data_35_6[["cogscore"]]
RMSE_data_35_6 <- sqrt(mean(error_p_data_35_6^2))

error_data_28_4 <- p_data_28_4 - data_35_6[["cogscore"]]
RMSE_data_28_4 <- sqrt(mean(error_data_28_4^2))

plot(error_test)
print(RMSE_test)

plot(error_data_35_6)
print(RMSE_data_35_6)

plot(error_data_28_4)
print(RMSE_data_28_4)
```








