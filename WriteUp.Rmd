---
title: "WriteUp"
author: "Group"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-data, echo = FALSE}
load("data/easySHARE_rel8_0_0.rda")
dat<-easySHARE_rel8_0_0
```

```{r packs, message = FALSE}
library(tidyverse)
library(gridExtra)
library(countrycode)
```

## Introduction

This investigation was carried out by Katrina Sanders, s1901708, Kim Johnson, s1936036 , and Max Guy, s1945362. 

This report explores risk factors of obesity using the easySHARE data. 
SHARE is the Survey of Health, Ageing and Retirment in Europe where easySHARE is a simplified dataset which was adapted for student training and teaching from the original data.  This was collected based mostly in Europe, about all aspects of life over many years where individuals, couples or household were questioned. The main topics of this data were selected such as demographics, household composition, social support, childhood conditions and health, functional limitations indices and work & money.  The data has 107 variables with 412110 observations which was observed over 8 waves of surveys.

The main questions we have attempted to answer are:
1. Which of the variables are associated with bmi(an indicator of obesity)? 
2. Are the associations between risk factors and obesity the same for males and females?

Our report also considers the strength and nature of the associations between risk factors. 

In order to begin our analysis we chose to work a subset of the data which covered 1 wave and 4 countries. We made the decisions for these based on the quality of the data. Since in our initial data exploration we noticed that many values were missing we wanted to choose a wave and countries within in it which had the best quality of data. We judged this on which had the least amount of missing data and constructed a function like this...

From this we found wave 6 to be best along with countries 15, 16, 23, and 35 which represent Spain, Italy, ??, and Estonia respectively. Wave 6 was carried out in 2015 so the data is still fairly current as well as not being skewed by COVID-19, which may have occurred from data collected in 2020 onwards. 
This higher quality of data will mean we are able to make more accurate conclusions. 
After choosing our subset of the data we began with dealing with all the missing values. We did this by replacing them all with Na's so we could look into the data further. 


```{r sorting-df, echo = FALSE}
dat[dat < 0] <- NA

data_countries <- dat[dat$country %in% c(15, 16, 23, 35), ]

data_full <- data_countries[data_countries$wave %in% c(6), ]
```

```{r echo = FALSE}
variables <- length(data_full)
observations <- nrow(data_full)
```

From here we started very high level and looked at summary statistics. 
The first thing we noted of importance was the vast number of variables within the data and observations, in total there are `r variables` variables, and `r observations` observations. The large amount of data available made us confident in our wave and country selections. However, at this point we did notice how many NA values we have within the data in total. 

This lead us to disregarding these columns as they will be of no importance in our investigation. We also 



```{r}
data_full %>%
  ggplot(aes(x=bmi)) +
  geom_density(fill = "#69b3a2")
```



```{r bmi females vs males, echo=FALSE}
data_full %>%
  ggplot(aes(x=as.factor(chronic_mod), y=bmi)) +
  geom_boxplot(fill = "slateblue")
```

We also initially explored the difference in bmi between males and females. From this plot we can see that males have a bmi much more concentrated around their mean, however females have a wider spread of bmi values. 

```{r female-vs-male, echo = FALSE}
data_full %>%
  ggplot(aes(x = bmi, color = as.factor(female))) +
  geom_density(adjust = 1.5, alpha = 0.4) 
```

The next step in our analysis was to build up a linear model using factors that affect bmi. We did this by creating a function which analyses the variables that we need to include in our model based on their correlation with bmi. We then continued this process until we got to a point that adding in a new variable was no longer adding to the [quality] of our model. 
We did this in a number of steps which we will detail below.

As mentioned previously we found that many columns included a large number of NA values, and therefore we decided that if more than 15% of values in a column were NA's we would remove the full column. We did this in order to keep our data high quality. This resulted in removing ... ?? At this point the wave number and year were removed as both of these variables are the same for each observation, and also mergeid,  hhid, and coupleid as these are just indicator values which will have no effect on our linear model.

From here we have constructed a data frame we want to use to then complete our linear model analysis. The next step we took was to calculate the correlations of each variable to bmi and rank them, showing the top 5. Here we ignore both bmi, and bmi 2, as bmi 2 is just a grouped data version of bmi. We then look for the highest correlation from the other variables and add this column of data to a new data frame and remove it from our original. This new data frame is then used to make a linear model with the column and produce an r^2 value. The same process is then repeated (with the original data frame - the previous column selected) and another r^2 value is produced. This process keeps repeating, adding more and more variables until the value of r^2 for the nth iteration isn't significantly different to the value of r^2 for the (n-1)th iteration. At this point we know the extra variable we are adding isn't adding value to our model, and therefore we end up with our final linear model. 

In our case we arrived at the 

- NA's removed 

- note: during model selection human decisions rather than iterative until occurrance of r^2


